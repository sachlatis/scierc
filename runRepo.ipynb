{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Init","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/sachlatis/fine-tune-bert-on-biomed.git\n%cd ./fine-tune-bert-on-biomed/data\n!unzip train.zip\n%cd ./../src","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-06T14:28:00.886071Z","iopub.execute_input":"2021-09-06T14:28:00.886475Z","iopub.status.idle":"2021-09-06T14:28:05.120459Z","shell.execute_reply.started":"2021-09-06T14:28:00.886438Z","shell.execute_reply":"2021-09-06T14:28:05.119522Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Cloning into 'fine-tune-bert-on-biomed'...\nremote: Enumerating objects: 41, done.\u001b[K\nremote: Counting objects: 100% (41/41), done.\u001b[K\nremote: Compressing objects: 100% (37/37), done.\u001b[K\nremote: Total 41 (delta 12), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (41/41), done.\n/kaggle/working/fine-tune-bert-on-biomed/src/fine-tune-bert-on-biomed/src/fine-tune-bert-on-biomed/src/fine-tune-bert-on-biomed/data\nArchive:  train.zip\n  inflating: train.txt               \n   creating: __MACOSX/\n  inflating: __MACOSX/._train.txt    \n/kaggle/working/fine-tune-bert-on-biomed/src/fine-tune-bert-on-biomed/src/fine-tune-bert-on-biomed/src/fine-tune-bert-on-biomed/src\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"!python3 sentence_classifier.py --mode train ","metadata":{"execution":{"iopub.status.busy":"2021-09-06T14:28:07.961965Z","iopub.execute_input":"2021-09-06T14:28:07.962311Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"2021-09-06 14:28:09.610577: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\nThere are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\nDownloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 3.83MB/s]\nDownloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 30.7kB/s]\nDownloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 3.84MB/s]\nDownloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 623kB/s]\nDownloading: 100%|███████████████████████████| 440M/440M [00:09<00:00, 47.5MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nGPU INF: Tesla P100-PCIE-16GB\n\n======== Epoch 1 / 3 ========\nTraining started ...\n  Batch    40  of  2,784.    Elapsed: 0:00:21.\n  Batch    80  of  2,784.    Elapsed: 0:00:40.\n  Batch   120  of  2,784.    Elapsed: 0:01:00.\n  Batch   160  of  2,784.    Elapsed: 0:01:20.\n  Batch   200  of  2,784.    Elapsed: 0:01:40.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Eval Model","metadata":{}},{"cell_type":"code","source":"!python3 sentence_classifier.py --mode eval","metadata":{},"execution_count":null,"outputs":[]}]}